# -*- coding: utf-8 -*-
"""Cust. Behavior Analyst

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fU-qxyf031qbFT9YllqwWzXHUlg29kRg
"""

import pandas as pd
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df=pd.read_csv("user_behavior_dataset.csv")
df.head()

df.info()

df.isnull().sum()

# Pilih hanya kolom numerik
numerical_df = df.select_dtypes(include=['int64', 'float64']).drop(['User ID', 'User Behavior Class', 'Age'], axis=1)

# Buat matriks korelasi
correlation_matrix = numerical_df.corr()

# Visualisasi menggunakan heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', vmin=-1, vmax=1)
plt.title("Correlation Matrix")
plt.show()

cols_to_plot = ['Battery Drain (mAh/day)', 'Screen On Time (hours/day)', 'App Usage Time (min/day)', 'Data Usage (MB/day)']

for col in cols_to_plot:
    plt.figure(figsize=(8, 6))
    sns.violinplot(y=df[col], color='lightcoral')
    plt.title(f'Violin Plot of {col}')
    plt.xlabel('Value')
    plt.ylabel(col)
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Group by device model and gender, and count the number of users
device_gender_counts = df.groupby(['Device Model', 'Gender'])['User ID'].count().unstack()

# Fill NaN values with 0 (if a device model doesn't have a certain gender)
device_gender_counts = device_gender_counts.fillna(0)

# Define a color palette
colors = sns.color_palette("coolwarm", len(device_gender_counts.columns))

# Plot the donut chart for each device model
for device_model in device_gender_counts.index:
    gender_counts = device_gender_counts.loc[device_model]

    plt.figure(figsize=(8, 6))
    wedges, texts, autotexts = plt.pie(
        gender_counts,
        labels=gender_counts.index,
        autopct='%1.1f%%',
        startangle=90,
        colors=colors,
        wedgeprops=dict(width=0.3, edgecolor='w')  # For donut chart
    )

    # Customize text styles
    plt.setp(autotexts, size=10, color="black")
    plt.setp(texts, size=12)

    # Add a circle in the center for the donut effect
    center_circle = plt.Circle((0, 0), 0.70, fc='white')
    plt.gca().add_artist(center_circle)

    # Add title with custom formatting
    plt.title(
        f"Gender Distribution for Device Model: {device_model}",
        fontsize=14, color="darkblue"
    )

    # Equal aspect ratio ensures the pie is drawn as a circle
    plt.axis('equal')
    plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='User Behavior Class', palette='viridis')
plt.title('Distribution of User Behavior Classes', fontsize=16)
plt.xlabel('User Behavior Class', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.show()

!pip install -q tensorflow
import tensorflow as tf

X=df.drop(['User Behavior Class'],axis=1)
y=df['User Behavior Class']

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.8,random_state=0)

X_train.info()

X_test.info()

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
X_train = pd.get_dummies(X_train)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)  # Converts strings to integers
one_hot_encoder = OneHotEncoder(sparse_output=False) # Changed 'sparse' to 'sparse_output'
y_train_onehot = one_hot_encoder.fit_transform(y_train_encoded.reshape(-1, 1))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LeakyReLU,PReLU,ELU,ReLU
from tensorflow.keras.layers import Dropout

classifier=Sequential()

df.columns.value_counts().sum()

classifier = Sequential()
classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=X_train.shape[1]))  # Input layer
classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))  # Hidden layer
classifier.add(Dense(units=y_train_onehot.shape[1], kernel_initializer='uniform', activation='softmax'))

classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model_history=classifier.fit(X_train,y_train_onehot,validation_split=0.33,batch_size=10,epochs=100)

model_history.history.keys()

accuracy=model_history.history['accuracy']
val_accuracy=model_history.history['val_accuracy']
loss=model_history.history['loss']
val_loss=model_history.history['val_loss']

for epoch in range(len(accuracy)):
    print(f"Epoch {epoch+1}:")
    print(f"  Training Accuracy: {accuracy[epoch]:.4f}")
    print(f"  Validation Accuracy: {val_accuracy[epoch]:.4f}")
    print(f"  Training Loss: {loss[epoch]:.4f}")
    print(f"  Validation Loss: {val_loss[epoch]:.4f}\n")

import pandas as pd

# Membuat DataFrame untuk hasil pelatihan dan validasi
history_df = pd.DataFrame({
    'Epoch': range(1, len(accuracy) + 1),
    'Training Accuracy': accuracy,
    'Validation Accuracy': val_accuracy,
    'Training Loss': loss,
    'Validation Loss': val_loss
})

# Menampilkan tabel
print(history_df)

classifier.summary()

classifier.get_weights()

# summarize history for accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Predicting the Test set results
X_test = pd.get_dummies(X_test)
X_train, X_test = X_train.align(X_test, join='left', axis=1)
X_test = X_test.fillna(0)
y_pred = classifier.predict(X_test)
y_pred_classes = y_pred.argmax(axis=1)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred_classes)
cm



"""akurasi total"""

# Pastikan Anda sudah mengimpor LabelEncoder dan OneHotEncoder
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import numpy as np

# Encode label y_test menggunakan LabelEncoder
label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)  # Mengubah label menjadi angka

# Lakukan one-hot encoding pada y_test yang sudah di-encode
# Ganti 'sparse' dengan 'sparse_output'
one_hot_encoder = OneHotEncoder(sparse_output=False)  # Mengubah menjadi format one-hot encoding
y_test_onehot = one_hot_encoder.fit_transform(y_test_encoded.reshape(-1, 1))

# Sekarang, Anda dapat melakukan evaluasi pada data uji
test_loss, test_accuracy = classifier.evaluate(X_test, y_test_onehot, batch_size=10)

# Menampilkan hasil akurasi dan loss pada data uji
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Prediksi kelas pada data uji
y_pred = classifier.predict(X_test)

# Mengambil kelas yang diprediksi (kelas dengan probabilitas tertinggi)
y_pred_classes = y_pred.argmax(axis=1)

# Menghitung akurasi secara manual
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred_classes)

# Menampilkan hasil akurasi
print(f"Test Accuracy: {accuracy * 100:.2f}%")





















